import {
  LLM,
  type LLMService,
  type GenerateResult,
  LLMError,
  type LLMRateLimitError,
} from '@repo/ai';
import { Layer, Effect } from 'effect';

/**
 * Default mock script output for podcast generation tests.
 */
export const DEFAULT_MOCK_SCRIPT = {
  title: 'Test Podcast Title',
  description: 'A test podcast description generated by the mock LLM.',
  summary: 'This is a test summary of the podcast content.',
  tags: ['test', 'podcast', 'mock'],
  segments: [
    {
      speaker: 'host',
      line: 'Welcome to the show! Today we have an exciting topic.',
      index: 0,
    },
    {
      speaker: 'cohost',
      line: 'Thanks for having me. I am looking forward to discussing this.',
      index: 1,
    },
    {
      speaker: 'host',
      line: 'Let us dive right into the key points.',
      index: 2,
    },
    { speaker: 'cohost', line: 'That is a great place to start.', index: 3 },
  ],
};

/**
 * Options for creating a mock LLM service.
 */
export interface MockLLMOptions {
  /**
   * Simulated delay in milliseconds before returning.
   */
  delay?: number;

  /**
   * Custom response object to return instead of the default.
   */
  response?: unknown;

  /**
   * Token usage to report.
   */
  usage?: {
    inputTokens: number;
    outputTokens: number;
    totalTokens: number;
  };

  /**
   * If set, the generate method will fail with this error message.
   */
  errorMessage?: string;
}

/**
 * Create a mock LLM layer for testing.
 *
 * @example
 * ```ts
 * const MockLLM = createMockLLM({ delay: 100 });
 *
 * await Effect.runPromise(
 *   generator.generateScript(podcastId).pipe(
 *     Effect.provide(MockLLM)
 *   )
 * );
 * ```
 */
export const createMockLLM = (
  options: MockLLMOptions = {},
): Layer.Layer<LLM> => {
  const service: LLMService = {
    model: { modelId: 'mock-model' },
    generate: <T>(): Effect.Effect<
      GenerateResult<T>,
      LLMError | LLMRateLimitError
    > =>
      Effect.gen(function* () {
        if (options.delay) {
          yield* Effect.sleep(options.delay);
        }

        if (options.errorMessage) {
          return yield* Effect.fail(
            new LLMError({ message: options.errorMessage }),
          );
        }

        return {
          object: (options.response ?? DEFAULT_MOCK_SCRIPT) as T,
          usage: options.usage ?? {
            inputTokens: 100,
            outputTokens: 200,
            totalTokens: 300,
          },
        } as GenerateResult<T>;
      }),
  };

  return Layer.succeed(LLM, service);
};

/**
 * Default mock LLM layer with standard test responses.
 */
export const MockLLMLive = createMockLLM();
